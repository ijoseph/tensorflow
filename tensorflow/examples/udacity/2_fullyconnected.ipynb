{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kR-4eNdK6lYS"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 2\n",
    "------------\n",
    "\n",
    "Previously in `1_notmnist.ipynb`, we created a pickle with formatted datasets for training, development and testing on the [notMNIST dataset](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html).\n",
    "\n",
    "The goal of this assignment is to progressively train deeper and more accurate models using TensorFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load /home/isajosep/.ipython/standard_imports.py\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import isajosep_util\n",
    "import os\n",
    "\n",
    "#!echo 'import os'  >> /home/isajosep/.ipython/standard_imports.py\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import isajosep_util.data_frame_plotter\n",
    "sns.set_palette([v.value for v in isajosep_util.data_frame_plotter.AgilentColors])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config the matplotlib backend as plotting inline in IPython\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load /home/isajosep/.ipython/standard_imports.py\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "JLpLa8Jt7Vu4"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle\n",
    "from six.moves import range\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1HrCK6e17WzV"
   },
   "source": [
    "First reload the data we generated in `1_notmnist.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 19456,
     "status": "ok",
     "timestamp": 1449847956073,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "y3-cj1bpmuxc",
    "outputId": "0ddb1607-1fc4-4ddb-de28-6c7ab7fb0c33"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28) (200000,)\n",
      "Validation set (10000, 28, 28) (10000,)\n",
      "Test set (10000, 28, 28) (10000,)\n",
      "CPU times: user 180 ms, sys: 860 ms, total: 1.04 s\n",
      "Wall time: 1.03 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data_root = os.path.join(os.path.expanduser('~'), 'Data/Udacity_Tensorflow')\n",
    "\n",
    "pickle_file = os.path.join(data_root, 'notMNIST.pickle')\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "    save = pickle.load(f)\n",
    "    train_dataset = save['train_dataset']\n",
    "    train_labels = save['train_labels']\n",
    "    valid_dataset = save['valid_dataset']\n",
    "    valid_labels = save['valid_labels']\n",
    "    test_dataset = save['test_dataset']\n",
    "    test_labels = save['test_labels']\n",
    "    del save  # hint to help gc free up memory\n",
    "    print('Training set', train_dataset.shape, train_labels.shape)\n",
    "    print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "    print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L7aHrm6nGDMB"
   },
   "source": [
    "Reformat into a shape that's more adapted to the models we're going to train:\n",
    "- data as a flat matrix,\n",
    "- labels as float 1-hot encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 19723,
     "status": "ok",
     "timestamp": 1449847956364,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "IRSyYiIIGIzS",
    "outputId": "2ba0fc75-1487-4ace-a562-cf81cae82793"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 784) (200000, 10)\n",
      "Validation set (10000, 784) (10000, 10)\n",
      "Test set (10000, 784) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "    dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n",
    "    # Map 0 to [1.0, 0.0, 0.0 ...], 1 to [0.0, 1.0, 0.0 ...]\n",
    "    labels = (np.arange(num_labels) == labels[:, None]).astype(np.float32)\n",
    "    return dataset, labels\n",
    "\n",
    "\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nCLVqyQ5vPPH"
   },
   "source": [
    "We're first going to train a multinomial logistic regression using simple gradient descent.\n",
    "\n",
    "TensorFlow works like this:\n",
    "* First you describe the computation that you want to see performed: what the inputs, the variables, and the operations look like. These get created as nodes over a computation graph. This description is all contained within the block below:\n",
    "\n",
    "      with graph.as_default():\n",
    "          ...\n",
    "\n",
    "* Then you can run the operations on this graph as many times as you want by calling `session.run()`, providing it outputs to fetch from the graph that get returned. This runtime operation is all contained in the block below:\n",
    "\n",
    "      with tf.Session(graph=graph) as session:\n",
    "          ...\n",
    "\n",
    "Let's load all the data into TensorFlow and build the computation graph corresponding to our training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "Nfv39qvtvOl_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-10-c75f962717d1>:31: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# With gradient descent training, even this much data is prohibitive.\n",
    "# Subset the training data for faster turnaround.\n",
    "train_subset = 10000\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "    # Input data.\n",
    "    # Load the training, validation and test data into constants that are\n",
    "    # attached to the graph.\n",
    "    tf_train_dataset = tf.constant(train_dataset[:train_subset, :])\n",
    "    tf_train_labels = tf.constant(train_labels[:train_subset])\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "\n",
    "    # Variables.\n",
    "    # These are the parameters that we are going to be training. The weight\n",
    "    # matrix will be initialized using random values following a (truncated)\n",
    "    # normal distribution. The biases get initialized to zero.\n",
    "    weights = tf.Variable(\n",
    "        tf.truncated_normal([image_size * image_size, num_labels]))  # W matrices\n",
    "    biases = tf.Variable(tf.truncated_normal([num_labels]))  # b\n",
    "\n",
    "    # Training computation.\n",
    "    # We multiply the inputs with the weight matrix, and add biases. We compute\n",
    "    # the softmax and cross-entropy (it's one operation in TensorFlow, because\n",
    "    # it's very common, and it can be optimized). We take the average of this\n",
    "    # cross-entropy across all training examples: that's our loss.\n",
    "    logits = tf.matmul(tf_train_dataset, weights) + biases\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))\n",
    "\n",
    "    # Optimizer.\n",
    "    # We are going to find the minimum of this loss using gradient descent.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "\n",
    "    # Predictions for the training, validation, and test data.\n",
    "    # These are not part of training, but merely here so that we can report\n",
    "    # accuracy figures as we train.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_prediction = tf.nn.softmax(\n",
    "        tf.matmul(tf_valid_dataset, weights) + biases)\n",
    "    test_prediction = tf.nn.softmax(\n",
    "        tf.matmul(tf_test_dataset, weights) + biases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KQcL4uqISHjP"
   },
   "source": [
    "Let's run this computation and iterate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 9
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 57454,
     "status": "ok",
     "timestamp": 1449847994134,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "z2cjdenH869W",
    "outputId": "4c037ba1-b526-4d8e-e632-91e2a0333267"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cd33f13587f4ffa9f3438806cf16e3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=801), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at step 0: 18.241243\n",
      "Training accuracy: 10.2%\n",
      "Validation accuracy: 13.4%\n",
      "Loss at step 100: 2.333565\n",
      "Training accuracy: 71.7%\n",
      "Validation accuracy: 70.1%\n",
      "Loss at step 200: 1.879436\n",
      "Training accuracy: 74.5%\n",
      "Validation accuracy: 72.7%\n",
      "Loss at step 300: 1.632090\n",
      "Training accuracy: 75.6%\n",
      "Validation accuracy: 73.6%\n",
      "Loss at step 400: 1.466185\n",
      "Training accuracy: 76.4%\n",
      "Validation accuracy: 74.3%\n",
      "Loss at step 500: 1.344700\n",
      "Training accuracy: 77.0%\n",
      "Validation accuracy: 74.6%\n",
      "Loss at step 600: 1.250389\n",
      "Training accuracy: 77.7%\n",
      "Validation accuracy: 74.8%\n",
      "Loss at step 700: 1.174178\n",
      "Training accuracy: 77.9%\n",
      "Validation accuracy: 75.0%\n",
      "Loss at step 800: 1.110919\n",
      "Training accuracy: 78.4%\n",
      "Validation accuracy: 75.2%\n",
      "\n",
      "Test accuracy: 82.5%\n",
      "CPU times: user 3.77 s, sys: 2.63 s, total: 6.4 s\n",
      "Wall time: 6.91 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "num_steps = 801\n",
    "\n",
    "\n",
    "def accuracy(predictions, labels):\n",
    "    return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "            / predictions.shape[0])\n",
    "\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    # This is a one-time operation which ensures the parameters get initialized as\n",
    "    # we described in the graph: random weights for the matrix, zeros for the\n",
    "    # biases.\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized')\n",
    "    for step in tqdm.tqdm_notebook(range(num_steps)):\n",
    "        # Run the computations. We tell .run() that we want to run the optimizer,\n",
    "        # and get the loss value and the training predictions returned as numpy\n",
    "        # arrays.\n",
    "        _, l, predictions = session.run([optimizer, loss, train_prediction])\n",
    "        if (step % 100 == 0):\n",
    "            print('Loss at step %d: %f' % (step, l))\n",
    "            print('Training accuracy: %.1f%%' % accuracy(\n",
    "                predictions, train_labels[:train_subset, :]))\n",
    "            # Calling .eval() on valid_prediction is basically like calling run(), but\n",
    "            # just to get that one numpy array. Note that it recomputes all its graph\n",
    "            # dependencies.\n",
    "            print('Validation accuracy: %.1f%%' % accuracy(\n",
    "                valid_prediction.eval(), valid_labels))\n",
    "    print('Test accuracy: %.1f%%' % accuracy(\n",
    "        test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "Nfv39qvtvOl_"
   },
   "outputs": [],
   "source": [
    "# With gradient descent training, even this much data is prohibitive.\n",
    "# Subset the training data for faster turnaround.\n",
    "train_subset = 10000\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "    # Input data.\n",
    "    # Load the training, validation and test data into constants that are\n",
    "    # attached to the graph.\n",
    "    tf_train_dataset = tf.constant(train_dataset[:train_subset, :])\n",
    "    tf_train_labels = tf.constant(train_labels[:train_subset])\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "\n",
    "    # Variables.\n",
    "    # These are the parameters that we are going to be training. The weight\n",
    "    # matrix will be initialized using random values following a (truncated)\n",
    "    # normal distribution. The biases get initialized to zero.\n",
    "    weights = tf.Variable(\n",
    "        tf.truncated_normal([image_size * image_size, num_labels]))  # W matrices\n",
    "    biases = tf.Variable(tf.zeros([num_labels]))  # b\n",
    "\n",
    "    # Training computation.\n",
    "    # We multiply the inputs with the weight matrix, and add biases. We compute\n",
    "    # the softmax and cross-entropy (it's one operation in TensorFlow, because\n",
    "    # it's very common, and it can be optimized). We take the average of this\n",
    "    # cross-entropy across all training examples: that's our loss.\n",
    "    logits = tf.matmul(tf_train_dataset, weights) + biases\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))\n",
    "\n",
    "    # Optimizer.\n",
    "    # We are going to find the minimum of this loss using gradient descent.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "\n",
    "    # Predictions for the training, validation, and test data.\n",
    "    # These are not part of training, but merely here so that we can report\n",
    "    # accuracy figures as we train.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_prediction = tf.nn.softmax(\n",
    "        tf.matmul(tf_valid_dataset, weights) + biases)\n",
    "    test_prediction = tf.nn.softmax(\n",
    "        tf.matmul(tf_test_dataset, weights) + biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 9
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 57454,
     "status": "ok",
     "timestamp": 1449847994134,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "z2cjdenH869W",
    "outputId": "4c037ba1-b526-4d8e-e632-91e2a0333267"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "051b5d1910be46a59c70d6366728c1ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=801), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at step 0: 16.157709\n",
      "Training accuracy: 7.3%\n",
      "Validation accuracy: 11.6%\n",
      "Loss at step 100: 2.481986\n",
      "Training accuracy: 70.5%\n",
      "Validation accuracy: 69.4%\n",
      "Loss at step 200: 1.974478\n",
      "Training accuracy: 73.4%\n",
      "Validation accuracy: 72.1%\n",
      "Loss at step 300: 1.696538\n",
      "Training accuracy: 74.9%\n",
      "Validation accuracy: 73.2%\n",
      "Loss at step 400: 1.510794\n",
      "Training accuracy: 76.0%\n",
      "Validation accuracy: 73.8%\n",
      "Loss at step 500: 1.375333\n",
      "Training accuracy: 76.7%\n",
      "Validation accuracy: 74.3%\n",
      "Loss at step 600: 1.271052\n",
      "Training accuracy: 77.3%\n",
      "Validation accuracy: 74.7%\n",
      "Loss at step 700: 1.187424\n",
      "Training accuracy: 78.0%\n",
      "Validation accuracy: 74.9%\n",
      "Loss at step 800: 1.118297\n",
      "Training accuracy: 78.6%\n",
      "Validation accuracy: 75.1%\n",
      "\n",
      "Test accuracy: 82.1%\n",
      "CPU times: user 3.22 s, sys: 1.77 s, total: 4.99 s\n",
      "Wall time: 4.33 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "num_steps = 801\n",
    "\n",
    "\n",
    "def accuracy(predictions, labels):\n",
    "    return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "            / predictions.shape[0])\n",
    "\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    # This is a one-time operation which ensures the parameters get initialized as\n",
    "    # we described in the graph: random weights for the matrix, zeros for the\n",
    "    # biases.\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized')\n",
    "    for step in tqdm.tqdm_notebook(range(num_steps)):\n",
    "        # Run the computations. We tell .run() that we want to run the optimizer,\n",
    "        # and get the loss value and the training predictions returned as numpy\n",
    "        # arrays.\n",
    "        _, l, predictions = session.run([optimizer, loss, train_prediction])\n",
    "        if (step % 100 == 0):\n",
    "            print('Loss at step %d: %f' % (step, l))\n",
    "            print('Training accuracy: %.1f%%' % accuracy(\n",
    "                predictions, train_labels[:train_subset, :]))\n",
    "            # Calling .eval() on valid_prediction is basically like calling run(), but\n",
    "            # just to get that one numpy array. Note that it recomputes all its graph\n",
    "            # dependencies.\n",
    "            print('Validation accuracy: %.1f%%' % accuracy(\n",
    "                valid_prediction.eval(), valid_labels))\n",
    "    print('Test accuracy: %.1f%%' % accuracy(\n",
    "        test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x68f-hxRGm3H"
   },
   "source": [
    "Let's now switch to stochastic gradient descent training instead, which is much faster.\n",
    "\n",
    "The graph will be similar, except that instead of holding all the training data into a constant node, we create a `Placeholder` node which will be fed actual data at every call of `session.run()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "qhPMzWYRGrzM"
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "    # Input data. For the training data, we use a placeholder that will be fed\n",
    "    # at run time with a training minibatch.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                      shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(\n",
    "        tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "\n",
    "    # Variables.\n",
    "    weights = tf.Variable(\n",
    "        tf.truncated_normal([image_size * image_size, num_labels]))\n",
    "    biases = tf.Variable(tf.zeros([num_labels]))\n",
    "\n",
    "    # Training computation.\n",
    "    logits = tf.matmul(tf_train_dataset, weights) + biases\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))\n",
    "\n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "\n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_prediction = tf.nn.softmax(\n",
    "        tf.matmul(tf_valid_dataset, weights) + biases)\n",
    "    test_prediction = tf.nn.softmax(\n",
    "        tf.matmul(tf_test_dataset, weights) + biases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XmVZESmtG4JH"
   },
   "source": [
    "Let's run it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 6
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 66292,
     "status": "ok",
     "timestamp": 1449848003013,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "FoF91pknG_YW",
    "outputId": "d255c80e-954d-4183-ca1c-c7333ce91d0a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc8937f463d24ae28dbffe442570198b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3001), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch loss at step 0: 14.653127\n",
      "Minibatch accuracy: 11.7%\n",
      "Validation accuracy: 16.9%\n",
      "Minibatch loss at step 500: 2.201866\n",
      "Minibatch accuracy: 73.4%\n",
      "Validation accuracy: 75.1%\n",
      "Minibatch loss at step 1000: 1.459991\n",
      "Minibatch accuracy: 75.8%\n",
      "Validation accuracy: 76.8%\n",
      "Minibatch loss at step 1500: 1.165973\n",
      "Minibatch accuracy: 77.3%\n",
      "Validation accuracy: 78.1%\n",
      "Minibatch loss at step 2000: 1.112722\n",
      "Minibatch accuracy: 78.1%\n",
      "Validation accuracy: 77.3%\n",
      "Minibatch loss at step 2500: 1.361714\n",
      "Minibatch accuracy: 76.6%\n",
      "Validation accuracy: 78.8%\n",
      "Minibatch loss at step 3000: 0.935472\n",
      "Minibatch accuracy: 77.3%\n",
      "Validation accuracy: 78.8%\n",
      "\n",
      "Test accuracy: 86.4%\n",
      "CPU times: user 7.87 s, sys: 1.73 s, total: 9.6 s\n",
      "Wall time: 6.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in tqdm.tqdm_notebook(range(num_steps)):\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {tf_train_dataset: batch_data,\n",
    "                     tf_train_labels: batch_labels}\n",
    "        _, l, predictions = session.run(\n",
    "            [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step % 500 == 0):\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" %\n",
    "                  accuracy(predictions, batch_labels))\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "                valid_prediction.eval(), valid_labels))\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(\n",
    "        test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7omWxtvLLxik"
   },
   "source": [
    "---\n",
    "Problem\n",
    "-------\n",
    "\n",
    "Turn the logistic regression example with SGD into a 1-hidden layer neural network with rectified linear units [nn.relu()](https://www.tensorflow.org/versions/r0.7/api_docs/python/nn.html#relu) and 1024 hidden nodes. This model should improve your validation / test accuracy.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # Input data\n",
    "    # For the training data, we use a placeholder that will be fed at run time with a training minibatch.\n",
    "    tf_train_dataset = tf.placeholder(\n",
    "        tf.float32, shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(\n",
    "        tf.float32, shape=(batch_size, num_labels))\n",
    "\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "\n",
    "    # Variables. (Parameters); initialize.\n",
    "    weights_layer_one = tf.Variable(tf.truncated_normal(\n",
    "        [image_size * image_size, image_size * image_size]))\n",
    "    biases_layer_one = tf.Variable(tf.zeros([image_size * image_size]))\n",
    "\n",
    "    weights_layer_three = tf.Variable(\n",
    "        tf.truncated_normal([image_size * image_size, num_labels]))\n",
    "    biases_layer_three = tf.Variable(tf.zeros([num_labels]))\n",
    "\n",
    "    # Training computation.\n",
    "    # Network architecture: 1024 RELUs\n",
    "    layer_one_output_tensor = tf.matmul(\n",
    "        tf_train_dataset, weights_layer_one) + biases_layer_one\n",
    "    layer_two_output_tensor = tf.nn.relu(layer_one_output_tensor)\n",
    "    logits = tf.matmul(layer_two_output_tensor,\n",
    "                       weights_layer_three) + biases_layer_three\n",
    "\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "        labels=tf_train_labels, logits=logits))\n",
    "\n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "\n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_prediction = tf.nn.softmax(\n",
    "        tf.matmul(tf.nn.relu(tf.matmul(tf_valid_dataset, weights_layer_one) + biases_layer_one), weights_layer_three) + biases_layer_three)\n",
    "    test_prediction = tf.nn.softmax(\n",
    "        tf.matmul(tf.nn.relu(tf.matmul(tf_test_dataset, weights_layer_one) + biases_layer_one), weights_layer_three) + biases_layer_three)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 6
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 66292,
     "status": "ok",
     "timestamp": 1449848003013,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "FoF91pknG_YW",
    "outputId": "d255c80e-954d-4183-ca1c-c7333ce91d0a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90e031f75d504810afca7247ec0a9505",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3001), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch loss at step 0: 355.677246\n",
      "Minibatch accuracy: 5.5%\n",
      "Validation accuracy: 34.3%\n",
      "Minibatch loss at step 500: 15.192797\n",
      "Minibatch accuracy: 74.2%\n",
      "Validation accuracy: 80.2%\n",
      "Minibatch loss at step 1000: 7.469962\n",
      "Minibatch accuracy: 75.8%\n",
      "Validation accuracy: 81.1%\n",
      "Minibatch loss at step 1500: 5.881219\n",
      "Minibatch accuracy: 75.8%\n",
      "Validation accuracy: 79.9%\n",
      "Minibatch loss at step 2000: 3.882264\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 78.7%\n",
      "Minibatch loss at step 2500: 5.214714\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 78.4%\n",
      "Minibatch loss at step 3000: 2.785893\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 81.4%\n",
      "\n",
      "Test accuracy: 88.8%\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 216x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 180,
       "width": 185
      }
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.6 s, sys: 2.18 s, total: 11.8 s\n",
      "Wall time: 7.98 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in tqdm.tqdm_notebook(range(num_steps)):\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {tf_train_dataset: batch_data,\n",
    "                     tf_train_labels: batch_labels}\n",
    "        _, l, predictions = session.run(\n",
    "            [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step % 500 == 0):\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" %\n",
    "                  accuracy(predictions, batch_labels))\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "                valid_prediction.eval(), valid_labels))\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(\n",
    "        test_prediction.eval(), test_labels))\n",
    "    tst_pct = accuracy(\n",
    "        test_prediction.eval(), test_labels) * .01\n",
    "    isajosep_util.plot_pie_percent(tst_pct)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "⇒ better! 81/88"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "2_fullyconnected.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
